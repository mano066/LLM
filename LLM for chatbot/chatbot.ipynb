{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1553e0a-e1ce-4f31-b3c0-edea7c021910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da451e40-bb84-46bd-81d6-a8a64a78ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "system_messages = [\n",
    "    \"You are a helpful assistant\",  # Default message\n",
    "    \"You are a friendly advisor\",\n",
    "    \"You are a knowledgeable guide\",\n",
    "    \"You are a witty companion\"\n",
    "]\n",
    "\n",
    "# Internal model names with user-friendly display names\n",
    "models = {\n",
    "    \"phi-3.1-mini-128k-instruct\": \"Phi (Microsoft)\",\n",
    "    \"granite-3.0-2b-instruct\": \"Granite (IBM)\",\n",
    "    \"gemma-2-2b-instruct\": \"Gemma (Google)\",\n",
    "    \"llama-3.2-3b-instruct\": \"LLaMA (Meta)\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7c5ed0-9352-4f9c-b015-5be550272fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting internal model names and their corresponding display names\n",
    "internal_models = list(models.keys())  # Internal names for API\n",
    "display_models = list(models.values())  # Display names for the dropdown\n",
    "\n",
    "def chat(message, selected_model_display, selected_system_message, history=None):\n",
    "    # Initialize history if None\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Map the display name back to the internal model name\n",
    "    selected_model = [k for k, v in models.items() if v == selected_model_display][0]\n",
    "\n",
    "    # Prepare the messages for the API call\n",
    "    messages = [{\"role\": \"system\", \"content\": selected_system_message}]  # Use selected system message\n",
    "    \n",
    "    # Append all history messages in the correct format\n",
    "    for user_message, assistant_message in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "    # Append the current user message\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Create a chat completion request\n",
    "    try:\n",
    "        response = client.chat.completions.create(model=selected_model, messages=messages)\n",
    "        assistant_response = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Sorry, I encountered an error. Please try again.\", history\n",
    "\n",
    "    # Update history with the new message and response\n",
    "    history.append((message, assistant_response))  # Ensure history is updated correctly\n",
    "    \n",
    "    return history, history  # Return updated history for the chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb8130b-ac2c-4b32-b3a4-b8673bb8b250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iei1\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\components\\chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7899\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7899/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clear_chat():\n",
    "    \"\"\"Clear the chat history.\"\"\"\n",
    "    return [], []  # Return empty history and state\n",
    "\n",
    "# Set up the Gradio interface\n",
    "with gr.Blocks() as iface:\n",
    "    gr.Markdown(\"<h1 style='text-align: center;'>Chatbot Interface</h1>\")\n",
    "    gr.Markdown(\"<p style='text-align: center;'>Choose a model and start chatting with the assistant!</p>\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        model_selection = gr.Dropdown(\n",
    "            choices=display_models,  # Use the display names for the dropdown\n",
    "            label=\"Select Model\",\n",
    "            value=display_models[0],  # Default selection\n",
    "            interactive=True\n",
    "        )\n",
    "        \n",
    "        system_message_selection = gr.Dropdown(\n",
    "            choices=system_messages,  # System messages dropdown\n",
    "            label=\"Select System Message\",\n",
    "            value=system_messages[0],  # Default selection\n",
    "            interactive=True\n",
    "        )\n",
    "\n",
    "        clear_button = gr.Button(\"Clear\")  # Button to clear the chat\n",
    "\n",
    "    chatbot = gr.Chatbot()  # Chatbot for displaying history\n",
    "    user_input = gr.Textbox(placeholder=\"Type your message here...\", label=\"Your Message\")  # Textbox for user input\n",
    "    state = gr.State([])  # Maintain chat history as state\n",
    "\n",
    "    # Define the function that gets triggered on user message submission\n",
    "    user_input.submit(chat, inputs=[user_input, model_selection, system_message_selection, state], outputs=[chatbot, state])\n",
    "    user_input.submit(lambda: \"\", outputs=user_input)  # Clear the input box after submission\n",
    "    clear_button.click(clear_chat, outputs=[chatbot, state])  # Connect the clear button to clear_chat function\n",
    "\n",
    "iface.launch(server_name=\"0.0.0.0\", server_port=7899)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c348ba-d107-47f0-80a9-af8605fbef5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
