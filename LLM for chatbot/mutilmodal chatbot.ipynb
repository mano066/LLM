{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d31aca7",
   "metadata": {},
   "source": [
    "# Building a Custom AI Chatbot Interface using LM Studio API and Gradio\n",
    "\n",
    "In this cell, we import the necessary libraries:\n",
    "- `os`: Provides functionalities for interacting with the operating system.\n",
    "- `openai`: The client library for interacting with OpenAI API.\n",
    "- `gradio`: A Python library used to build and launch web-based applications, here used to build the chatbot interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1553e0a-e1ce-4f31-b3c0-edea7c021910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import gradio as gr                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4059d0f",
   "metadata": {},
   "source": [
    "## Installing LM Studio and Loading Models\n",
    "\n",
    "1. **Download and Install LM Studio**: Follow the instructions provided by LM Studio to install it locally.\n",
    "2. **Download Models**: Choose and download the required language models from the LM Studio repository.\n",
    "3. **Start the LM Studio Server**: Run the LM Studio server on `http://localhost:1234` to handle API requests.\n",
    "\n",
    "Here, the OpenAI client is initialized by the base URL where the LM Studio API is running (on localhost). The `api_key` is also provided. This client will be used to make API requests for the chatbot's responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da451e40-bb84-46bd-81d6-a8a64a78ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc15478",
   "metadata": {},
   "source": [
    "\n",
    "This cell defines two sets of options for user selection:\n",
    "1. **System Messages**: Predefined system prompts to guide the chatbot's behavior, such as \"You are a helpful assistant.\"\n",
    "2. **Models**: Available AI models for inference. Internal names (used for API requests) are mapped to user-friendly display names. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39085a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant. \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f275b",
   "metadata": {},
   "source": [
    "\n",
    "The `chat` function handles interaction with the LM Studio API. It prepares a conversation history, which includes user inputs and assistant responses. \n",
    "\n",
    "* The `chat` function constructs a list of messages, including the system message, previous conversation history, and the user's current message.\n",
    "* It then sends this list to a chatbot model (`phi-3.1-mini-128k-instruct`) using the `client.chat.completions.create` method.\n",
    "* The function returns the chatbot's response.\n",
    "* The `gr.ChatInterface` function is used to launch a chat interface with the `chat` function as its backend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7c5ed0-9352-4f9c-b015-5be550272fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    for human, assistant in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": human})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    response = client.chat.completions.create(model=\"phi-3.1-mini-128k-instruct\", messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "gr.ChatInterface(fn=chat).launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c348ba-d107-47f0-80a9-af8605fbef5d",
   "metadata": {},
   "source": [
    "# image generator Modal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8aa97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c42541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "def artist(prompt, output_path=None, model_id=\"CompVis/stable-diffusion-v1-1\"):\n",
    "    # Check if a GPU is available and use it if possible\n",
    "    device = \"cuda\"\n",
    "\n",
    "    # Load the pre-trained Stable Diffusion model\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "    pipe = pipe.to(device)\n",
    "\n",
    "    # Generate the image\n",
    "    with torch.autocast(device):\n",
    "        image = pipe(prompt).images[0]  # Generate image based on the prompt\n",
    "\n",
    "    # Save the generated image if output path is specified\n",
    "    if output_path:\n",
    "        image.save(output_path)\n",
    "        #print(f\"Image saved as '{output_path}'\")\n",
    "\n",
    "    # Display the image in Jupyter Notebook\n",
    "    display.display(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "artist(\"Create a new alien city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46c405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4ec6378",
   "metadata": {},
   "source": [
    "# Add Image model in chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61fad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize OpenAI client for chat\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Define system message for chat\n",
    "system_message = \"You are a helpful assistant.\"\n",
    "\n",
    "# Global variable for the Stable Diffusion model\n",
    "pipe = None\n",
    "\n",
    "# List of keywords that trigger image generation\n",
    "image_generation_keywords = ['create', 'generate', 'draw', 'image', 'picture', 'illustrate']\n",
    "\n",
    "# Chat function using OpenAI\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    for human, assistant in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": human})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Make chat request to OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"phi-3.1-mini-128k-instruct\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    # Return assistant's reply\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Image generation function using Stable Diffusion\n",
    "def artist(prompt, model_id=\"CompVis/stable-diffusion-v1-1\"):\n",
    "    global pipe\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    if pipe is None:  # Load the model only once\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n",
    "        pipe = pipe.to(device)\n",
    "\n",
    "    try:\n",
    "        with torch.autocast(device if device == \"cuda\" else \"cpu\"):\n",
    "            image = pipe(prompt).images[0]  # Generate image based on the prompt\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {str(e)}\")\n",
    "        image = Image.new('RGB', (512, 512), color='white')  # Placeholder image\n",
    "\n",
    "    return image\n",
    "\n",
    "# Combine chat and image generation process\n",
    "def chat_and_generate_image(user_message, history):\n",
    "    # Check if the message contains any keywords for image generation\n",
    "    if any(keyword in user_message.lower() for keyword in image_generation_keywords):\n",
    "        # Generate image based on user prompt\n",
    "        generated_image = artist(user_message)  # Use the user message as the prompt for image generation\n",
    "    else:\n",
    "        generated_image = Image.new('RGB', (512, 512), color='white')  # Placeholder image if no image is generated\n",
    "\n",
    "    # Chat response\n",
    "    chat_response = chat(user_message, history[:-1])\n",
    "    \n",
    "    return chat_response, generated_image\n",
    "\n",
    "# Build Gradio interface with Blocks\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        # Chatbot display\n",
    "        chatbot = gr.Chatbot(height=500)\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Input textbox for user message\n",
    "        msg = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Clear button\n",
    "        clear = gr.Button(\"Clear\")\n",
    "    \n",
    "    # Output for image display\n",
    "    image_output = gr.Image(type=\"pil\", height=512)\n",
    "\n",
    "    # User function to update history\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    # Bot function to handle chat and image generation\n",
    "    def bot(history):\n",
    "        user_message = history[-1][0]\n",
    "        bot_message, image = chat_and_generate_image(user_message, history)\n",
    "\n",
    "        # Append bot message to the history\n",
    "        history[-1][1] = bot_message\n",
    "        \n",
    "        return history, image  # Return history and generated image\n",
    "\n",
    "    # When user submits a message, update chatbot and then generate the bot response and image\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, [chatbot, image_output]  # Update chatbot and image output\n",
    "    )\n",
    "\n",
    "    # Clear chat history on clicking 'Clear'\n",
    "    clear.click(lambda: (None, Image.new('RGB', (512, 512), color='white')), None, [chatbot, image_output], queue=False)\n",
    "\n",
    "# Launch the UI\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daa68b4",
   "metadata": {},
   "source": [
    "# audio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f78d40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash attention 2 is not installed\n",
      "c:\\Users\\iei1\\.conda\\envs\\llms\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import soundfile as sf\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n",
    "\n",
    "def talker(prompt):\n",
    "    try:\n",
    "        # Description of the voice characteristics (optional, can be customized)\n",
    "        description = \"Jon's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.\"\n",
    "        \n",
    "        # Tokenize the description and the prompt\n",
    "        input_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "        prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        # Generate the audio using the model\n",
    "        generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "        \n",
    "        # Convert the numpy array to a BytesIO buffer\n",
    "        audio_arr = generation.cpu().numpy().squeeze()\n",
    "        audio_buffer = BytesIO()\n",
    "        sf.write(audio_buffer, audio_arr, model.config.sampling_rate, format='WAV')\n",
    "        audio_buffer.seek(0)  # Seek to the start of the buffer\n",
    "        \n",
    "        # Load the audio into an AudioSegment and play it\n",
    "        audio_segment = AudioSegment.from_file(audio_buffer, format=\"wav\")\n",
    "        play(audio_segment)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "talker(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c0352",
   "metadata": {},
   "source": [
    "# Add image generation and audio generation (PARLER_TTS) model in chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e89183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iei1\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\components\\chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import soundfile as sf\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "import tempfile\n",
    "\n",
    "# Initialize OpenAI client for chat\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Define system message for chat\n",
    "system_message = \"You are a helpful assistant.\"\n",
    "system_message += \"Using stable-diffusion-v1-1 for generated image and parler-tts-mini-v1 use for audio generated.\"\n",
    "\n",
    "# Global variables for models\n",
    "pipe = None\n",
    "tts_model = None\n",
    "tokenizer = None\n",
    "\n",
    "# List of keywords that trigger image generation\n",
    "image_generation_keywords = ['create', 'generate', 'draw', 'image', 'picture', 'illustrate']\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load TTS model and tokenizer\n",
    "tts_model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n",
    "\n",
    "# Chat function using OpenAI\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    for human, assistant in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": human})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Make chat request to OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.2-3b-instruct\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    # Return assistant's reply\n",
    "    reply = response.choices[0].message.content\n",
    "    return reply\n",
    "\n",
    "# Image generation function using Stable Diffusion\n",
    "def artist(prompt, model_id=\"CompVis/stable-diffusion-v1-1\"):\n",
    "    global pipe\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    if pipe is None:  # Load the model only once\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n",
    "        pipe = pipe.to(device)\n",
    "\n",
    "    try:\n",
    "        with torch.autocast(device if device == \"cuda\" else \"cpu\"):\n",
    "            image = pipe(prompt).images[0]  # Generate image based on the prompt\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {str(e)}\")\n",
    "        image = Image.new('RGB', (512, 512), color='white')  # Placeholder image\n",
    "\n",
    "    return image\n",
    "\n",
    "# Talker function for generating audio\n",
    "def talker(prompt):\n",
    "    try:\n",
    "        # Create a temporary file to hold the audio data\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio_file:\n",
    "            audio_buffer = BytesIO()  # Create a buffer to hold audio data\n",
    "            \n",
    "            # Description of the voice characteristics\n",
    "            description = \"Jon's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.\"\n",
    "            \n",
    "            # Tokenize the description and the prompt\n",
    "            input_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "            prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "            \n",
    "            # Generate the audio using the model\n",
    "            generation = tts_model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "            \n",
    "            # Convert the numpy array to a WAV file\n",
    "            audio_arr = generation.cpu().numpy().squeeze()\n",
    "            sf.write(temp_audio_file.name, audio_arr, tts_model.config.sampling_rate, format='WAV')\n",
    "            temp_audio_file.seek(0)  # Seek to the start of the file\n",
    "\n",
    "            audio_file_path = temp_audio_file.name  # Get the file path\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        audio_file_path = None  # Handle error case\n",
    "\n",
    "    return audio_file_path   # Return the audio buffer\n",
    "\n",
    "# Combine chat and image generation process\n",
    "def chat_and_generate_image(user_message, history):\n",
    "    # Check if the message contains any keywords for image generation\n",
    "    if any(keyword in user_message.lower() for keyword in image_generation_keywords):\n",
    "        # Generate image based on user prompt\n",
    "        generated_image = artist(user_message)  # Use the user message as the prompt for image generation\n",
    "    else:\n",
    "        generated_image = Image.new('RGB', (512, 512), color='white')  # Placeholder image if no image is generated\n",
    "\n",
    "    # Chat response\n",
    "    chat_response = chat(user_message, history[:-1])\n",
    "    \n",
    "    # Generate audio for the chat response\n",
    "    audio_buffer = talker(chat_response)\n",
    "    \n",
    "    return chat_response, generated_image, audio_buffer\n",
    "\n",
    "# Build Gradio interface with Blocks\n",
    "with gr.Blocks() as ui:\n",
    "    # Main title for the UI\n",
    "    gr.Markdown(\"<h1 style='text-align: center;'>AI Assistant with Image Generation and Voice</h1>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # Left column for chatbot display\n",
    "        with gr.Column(scale=1):  # Adjust the scale for desired width\n",
    "            # Chatbot display\n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "\n",
    "            # Input textbox for user message\n",
    "            msg = gr.Textbox(label=\"Chat with our AI Assistant:\", placeholder=\"Type your message here...\")\n",
    "\n",
    "            # Clear button\n",
    "            clear = gr.Button(\"Clear\")\n",
    "\n",
    "        # Right column for image display\n",
    "        with gr.Column(scale=1):  # Adjust the scale for desired width\n",
    "            # Output for image display\n",
    "            image_output = gr.Image(type=\"pil\", height=512, label=\"Generated Image\")\n",
    "\n",
    "            # Output for audio playback\n",
    "            audio_output = gr.Audio(label=\"Generated Audio\")\n",
    "\n",
    "    # User function to update history\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    # Bot function to handle chat and image generation\n",
    "    def bot(history):\n",
    "        user_message = history[-1][0]\n",
    "        bot_message, image, audio = chat_and_generate_image(user_message, history)\n",
    "\n",
    "        # Append bot message to the history\n",
    "        history[-1][1] = bot_message\n",
    "        \n",
    "        return history, image, audio  # Return history, generated image, and audio buffer\n",
    "\n",
    "    # When user submits a message, update chatbot and then generate the bot response and image\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, [chatbot, image_output, audio_output]  # Update chatbot, image output, and audio output\n",
    "    )\n",
    "\n",
    "    # Clear chat history on clicking 'Clear'\n",
    "    clear.click(lambda: (None, Image.new('RGB', (512, 512), color='white'), None), None, [chatbot, image_output, audio_output], queue=False)\n",
    "\n",
    "# Launch the UI\n",
    "ui.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab2829b",
   "metadata": {},
   "source": [
    "# Add image generation and audio generation (microsoft_TTS) only 600 tokens model in chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd3ca97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iei1\\.conda\\envs\\llms\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "c:\\Users\\iei1\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\components\\chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unet\\diffusion_pytorch_model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922c5e678e574fde97534e0e36db2d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch C:\\Users\\iei1\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-1\\snapshots\\574555a657f0c8d6e970a02c7e095e9d4220dcbf\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\iei1\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-1\\snapshots\\574555a657f0c8d6e970a02c7e095e9d4220dcbf\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch C:\\Users\\iei1\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-1\\snapshots\\574555a657f0c8d6e970a02c7e095e9d4220dcbf\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\iei1\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-1\\snapshots\\574555a657f0c8d6e970a02c7e095e9d4220dcbf\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf7749bc6454ea79c8790ad8d5ff506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import soundfile as sf\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "import tempfile\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize OpenAI client for chat\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Define system message for chat\n",
    "system_message = \"You are a helpful assistant. Your responses should be clear and informative, limited to a maximum of 200 words. Focus on providing accurate and relevant information to user inquiries while maintaining a friendly and professional tone\"\n",
    "system_message += \"Using stable-diffusion-v1-1 for generated image and parler-tts-mini-v1 use for audio generated.\"\n",
    "\n",
    "# Global variables for models\n",
    "pipe = None\n",
    "tts_model = None\n",
    "tokenizer = None\n",
    "\n",
    "# List of keywords that trigger image generation\n",
    "image_generation_keywords = ['create', 'generate', 'draw', 'image', 'picture', 'illustrate']\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load TTS model and tokenizer\n",
    "tts_model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n",
    "\n",
    "# Chat function using OpenAI\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    for human, assistant in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": human})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Make chat request to OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gemma-2-2b-it\",\n",
    "        messages=messages,\n",
    "        max_tokens= 200\n",
    "    )\n",
    "    \n",
    "    # Return assistant's reply\n",
    "    reply = response.choices[0].message.content\n",
    "    return reply\n",
    "\n",
    "# Image generation function using Stable Diffusion\n",
    "def artist(prompt, model_id=\"CompVis/stable-diffusion-v1-1\"):\n",
    "    global pipe\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    if pipe is None:  # Load the model only once\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n",
    "        pipe = pipe.to(device)\n",
    "\n",
    "    try:\n",
    "        with torch.autocast(device if device == \"cuda\" else \"cpu\"):\n",
    "            image = pipe(prompt).images[0]  # Generate image based on the prompt\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {str(e)}\")\n",
    "        image = Image.new('RGB', (512, 512), color='white')  # Placeholder image\n",
    "\n",
    "    return image\n",
    "\n",
    "# Talker function for generating audio\n",
    "def talker(prompt, speaker_index=7306, device='cuda'):\n",
    "    try:\n",
    "        # Load the TTS synthesizer\n",
    "        synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device=device)\n",
    "        \n",
    "        # Load the speaker embeddings dataset\n",
    "        embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "        speaker_embedding = torch.tensor(embeddings_dataset[speaker_index][\"xvector\"]).unsqueeze(0)\n",
    "        \n",
    "        # Generate speech with the specified prompt and speaker embedding\n",
    "        speech = synthesiser(prompt, forward_params={\"speaker_embeddings\": speaker_embedding})\n",
    "\n",
    "        # Create a temporary file to hold the audio data\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio_file:\n",
    "            # Save the generated audio to a file\n",
    "            sf.write(temp_audio_file.name, speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
    "            temp_audio_file.seek(0)  # Seek to the start of the file\n",
    "            \n",
    "            audio_file_path = temp_audio_file.name  # Get the file path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        audio_file_path = None  # Handle error case\n",
    "\n",
    "    return audio_file_path# Return the audio buffer\n",
    "\n",
    "\n",
    "# Combine chat and image generation process\n",
    "def chat_and_generate_image(user_message, history):\n",
    "    # Check if the message contains any keywords for image generation\n",
    "    if any(keyword in user_message.lower() for keyword in image_generation_keywords):\n",
    "        # Generate image based on user prompt\n",
    "        generated_image = artist(user_message)  # Use the user message as the prompt for image generation\n",
    "    else:\n",
    "        generated_image = Image.new('RGB', (512, 512), color='white')  # Placeholder image if no image is generated\n",
    "\n",
    "    # Chat response\n",
    "    chat_response = chat(user_message, history[:-1])\n",
    "    \n",
    "    # Generate audio for the chat response\n",
    "    audio_buffer = talker(chat_response)\n",
    "    \n",
    "    return chat_response, generated_image, audio_buffer\n",
    "\n",
    "# Build Gradio interface with Blocks\n",
    "with gr.Blocks() as ui:\n",
    "    # Main title for the UI\n",
    "    gr.Markdown(\"<h1 style='text-align: center;'>AI Assistant with Image Generation and Voice</h1>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # Left column for chatbot display\n",
    "        with gr.Column(scale=1):  # Adjust the scale for desired width\n",
    "            # Chatbot display\n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "\n",
    "            # Input textbox for user message\n",
    "            msg = gr.Textbox(label=\"Chat with our AI Assistant:\", placeholder=\"Type your message here...\")\n",
    "\n",
    "            # Clear button\n",
    "            clear = gr.Button(\"Clear\")\n",
    "\n",
    "        # Right column for image display\n",
    "        with gr.Column(scale=1):  # Adjust the scale for desired width\n",
    "            # Output for image display\n",
    "            image_output = gr.Image(type=\"pil\", height=512, label=\"Generated Image\")\n",
    "\n",
    "            # Output for audio playback\n",
    "            audio_output = gr.Audio(label=\"Generated Audio\")\n",
    "\n",
    "    # User function to update history\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    # Bot function to handle chat and image generation\n",
    "    def bot(history):\n",
    "        user_message = history[-1][0]\n",
    "        bot_message, image, audio = chat_and_generate_image(user_message, history)\n",
    "\n",
    "        # Append bot message to the history\n",
    "        history[-1][1] = bot_message\n",
    "        \n",
    "        return history, image, audio  # Return history, generated image, and audio buffer\n",
    "\n",
    "    # When user submits a message, update chatbot and then generate the bot response and image\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, [chatbot, image_output, audio_output]  # Update chatbot, image output, and audio output\n",
    "    )\n",
    "\n",
    "    # Clear chat history on clicking 'Clear'\n",
    "    clear.click(lambda: (None, Image.new('RGB', (512, 512), color='white'), None), None, [chatbot, image_output, audio_output], queue=False)\n",
    "\n",
    "# Launch the UI\n",
    "ui.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c29878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
