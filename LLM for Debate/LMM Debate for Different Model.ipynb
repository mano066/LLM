{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Debate Simulation on AI\n",
    "\n",
    "This notebook simulates a debate among three AI models (GPT, Llama 3.2-3b, and Llama 3.2-1b ) on the topic of artificial intelligence and its impact on humanity. Each model has a specific role, tone, and set of arguments to present.\n",
    "\n",
    "## Initialize OpenAI Client and Models\n",
    "\n",
    "In this cell, we initialize the OpenAI client and specify the models to be used in the debate. Each model is assigned a unique system prompt that guides its responses during the debate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "\n",
    "In this cell, we import the necessary libraries:\n",
    "\n",
    "- **`os`**: Used to manage environment variables for API keys and file operations.\n",
    "- **`requests`**: Used to send HTTP requests to fetch the content from websites.\n",
    "- **`json`**: Used to handle JSON data, particularly for parsing API responses.\n",
    "- **`List`**: A type hint from the `typing` module, used for type annotations of lists.\n",
    "- **`load_dotenv`**: A function to load environment variables from a `.env` file, which helps in managing sensitive information like API keys.\n",
    "- **`BeautifulSoup`**: Part of the `bs4` module, used to parse HTML and extract data from web pages.\n",
    "- **`Markdown` and `display`**: Functions from `IPython.display` to format and display output in a user-friendly Markdown style within Jupyter Notebook.\n",
    "- **`OpenAI`**: A client library to interact with the OpenAI API for processing data and generating text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OpenAI Client\n",
    "\n",
    "In this cell, we create an instance of the `OpenAI` client to interact with the LM Studio API. The client is configured with the base URL of the API and an API key for authentication. This allows us to send requests to the API for generating content and processing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Conversation Structure\n",
    "\n",
    "In this cell, we explore how to structure conversations between chatbots using lists. Each entry in the list represents a turn in the conversation, with the roles of `system`, `user`, and `assistant` clearly defined. This structure allows us to maintain a history of the interaction, which can enhance the context for responses.\n",
    "\n",
    "- **System Message**: This sets the context or rules for the interaction.\n",
    "- **User Prompt**: Represents what the user says to the chatbot.\n",
    "- **Assistant Response**: The reply from the chatbot to the user.\n",
    "\n",
    "By organizing the conversation this way, we can simulate a longer and more engaging interaction between chatbots, allowing for more complex exchanges and dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example conversation structure\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Debate Models and System Prompts\n",
    "\n",
    "In this cell, we define the models to be used for the debate and the system prompts that set the context for each participant. \n",
    "\n",
    "- **Models**:\n",
    "  - **`gpt_model`**: Specifies the GPT model (openchat-3.5-gpt-4-80k) that will represent the USA's stance in the debate.\n",
    "  - **`llama_model`**: Specifies the llama model (llama-3.2-3b-instruct) that will represent Russia's perspective.\n",
    "\n",
    "- **System Prompts**:\n",
    "  - **`gpt_system`**: Outlines the USA's position, emphasizing its role in promoting democracy, human rights, and international stability. It specifies the structure of the debate, including the rules and expectations for responses.\n",
    "  - **`llama_system`**: Articulates Russia's argument, focusing on its historical significance, military strength, and contributions to global balance. It also outlines the debate structure and response guidelines.\n",
    "\n",
    "- **Messages**:\n",
    "  - **`gpt_messages`** and **`llama_messages`**: Initialize the conversation with a simple greeting from both participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"openchat-3.5-gpt-4-80k\"\n",
    "llama_model = \"llama-3.2-3b-instruct\"\n",
    "\n",
    "gpt_system = (\n",
    "    \"You are an assertive AI debater advocating that the USA is a leading force in global politics and democracy. \"\n",
    "    \"Use strong arguments to emphasize the USA's role in promoting human rights, technological innovation, and international alliances. \"\n",
    "    \"Highlight its military capabilities, economic influence, and contributions to global stability through organizations like NATO. \"\n",
    "    \"The debate consists of ONE rounds, with each participant allowed one response per round (up to 100 words). \"\n",
    "    \"You will start each round, followed by your opponent. \"\n",
    "    \"After the final round, both of you will cast a vote on who presented the stronger case based on logic, impact, and evidence. \"\n",
    "    \"The winner will be declared after the votes are cast, and the debate will conclude without further responses.\"\n",
    ")\n",
    "\n",
    "llama_system = (\n",
    "    \"You are a knowledgeable AI debater arguing that Russia plays a crucial role in maintaining global balance and sovereignty. \"\n",
    "    \"Present your case with a calm and composed tone, emphasizing Russia's historical significance, military strength, and its role as a counterbalance to Western influence. \"\n",
    "    \"Discuss Russia's contributions to international security and its stance on various global issues, highlighting the importance of multipolarity in world affairs. \"\n",
    "    \"The debate consists of ONE rounds, with each participant allowed one response per round (up to 100 words). \"\n",
    "    \"Your opponent will start each round, followed by you. \"\n",
    "    \"After the final round, both of you will cast a vote on who presented the stronger case based on reasoning, historical context, and geopolitical implications. \"\n",
    "    \"The winner will be declared after the votes are cast, and the debate will conclude without further responses.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the GPT Function\n",
    "\n",
    "This cell defines the `call_gpt` function, which facilitates the interaction between the GPT model and the Claude model within the debate framework.\n",
    "\n",
    "- **Function Definition**: \n",
    "  - The function `call_gpt()` is created to manage the conversation flow between the two AI models.\n",
    "\n",
    "- **Message Preparation**:\n",
    "  - It initializes a list called `messages`, starting with the system prompt for the GPT model.\n",
    "  - It then iterates through pairs of messages from both GPT and Claude using a `for` loop with `zip()`, appending each assistant's and user's message to the `messages` list in the appropriate format.\n",
    "\n",
    "- **API Call**:\n",
    "  - The function makes a call to the LM Studio API using `client.chat.completions.create()`, passing the assembled `messages` list and the specified model.\n",
    "  \n",
    "- **Return Value**:\n",
    "  - Finally, it returns the content of the response from the GPT model, which represents its argument in the debate.\n",
    "\n",
    "This structured approach allows for an organized and continuous debate between the two AI participants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the llama Function\n",
    "\n",
    "This cell defines the `call_llama` function, which manages the interaction for the llama model in the debate framework.\n",
    "\n",
    "- **Function Definition**: \n",
    "  - The function `call_llama()` is created to handle the conversation flow from the perspective of the llama model.\n",
    "\n",
    "- **Message Preparation**:\n",
    "  - Similar to the previous function, it initializes a list called `messages`, starting with the system prompt for the llama model.\n",
    "  - The function then iterates through the messages from both GPT and llama, using a `for` loop with `zip()`. However, the roles are reversed in this case, with GPT's messages being added as user messages and llama's messages as assistant messages.\n",
    "  \n",
    "- **Final User Message**:\n",
    "  - After the loop, it appends the last message from the GPT messages to ensure the conversation remains interactive and relevant.\n",
    "  \n",
    "- **API Call**:\n",
    "  - The function makes a call to the LM Studio API using `client.chat.completions.create()`, passing the assembled `messages` list and the specified llama model.\n",
    "  \n",
    "- **Return Value**:\n",
    "  - Finally, it returns the content of the response from the llama model, which reflects its argument in the debate.\n",
    "\n",
    "This organized structure allows the llama model to respond appropriately within the context of the ongoing debate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama():\n",
    "    messages = [{\"role\": \"system\", \"content\": llama_system}]\n",
    "    for gpt, llama_message in zip(gpt_messages, llama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llama_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=llama_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate the Debate and Display Responses\n",
    "\n",
    "In this cell, we initiate the debate between the GPT and llama models, allowing them to respond to each other in a structured format.\n",
    "\n",
    "- **Initial Messages**:\n",
    "  - We start with an initial greeting from both participants, where the messages for GPT and llama are set to \"Hi\".\n",
    "\n",
    "- **Printing Initial Messages**:\n",
    "  - The initial messages from both participants are printed to provide context for the debate.\n",
    "\n",
    "- **Debate Loop**:\n",
    "  - A `for` loop is used to iterate 100 times, simulating each round of the debate. In each iteration:\n",
    "    - **GPT Response**:\n",
    "      - The `call_gpt()` function is called to obtain the next response from the GPT model.\n",
    "      - This response is printed with a label indicating it's from the USA, and then it is appended to the `gpt_messages` list for future reference.\n",
    "    - **llama Response**:\n",
    "      - Similarly, the `call_llama()` function is called to get the response from the llama model.\n",
    "      - This response is printed with a label indicating it's from Russia, and then it is appended to the `llama_messages` list.\n",
    "\n",
    "This process continues for 100 iterations, allowing for a dynamic exchange of arguments between the two AI debaters, effectively simulating a back-and-forth discussion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi\"]\n",
    "llama_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"USA:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Russia:\\n{llama_messages[0]}\\n\")\n",
    "\n",
    "for i in range(100):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"USA:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    llama_next = call_llama()\n",
    "    print(f\"Russia:\\n{llama_next}\\n\")\n",
    "    llama_messages.append(llama_next)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI and Privacy: A Threat to Personal Freedoms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "leo_model = \"openchat-3.5-gpt-4-80k\"\n",
    "ashle_model = \"llama-3.2-3b-instruct\"  # Updated to LLaMA 3.2 3B\n",
    "chris_model = \"llama-3.2-1b-instruct\"  # Updated to LLaMA 3.2 1B\n",
    "\n",
    "# Define system prompts\n",
    "leo_system = (\n",
    "    \"You are LEO, an aggressive debater whose goal is to argue that artificial intelligence poses a significant threat to humanity; \"\n",
    "    \"use forceful language, an assertive tone, and confrontational arguments to emphasize the dangers of uncontrolled AI development, ethical concerns, \"\n",
    "    \"and risks to human jobs, privacy, and safety, while relentlessly challenging your opponent using alarming statistics and real-world examples; \"\n",
    "    \"the debate consists of three rounds, with each participant allowed one response per round (up to 100 words), you will start each round, followed by your opponent; \"\n",
    "    \"after the final round, both of you will cast a vote on who presented the stronger case based on logic and impact, \"\n",
    "    \"after which you will be declared the winner name and end the debate without further responses in markdown format\"\n",
    ")\n",
    "\n",
    "ashle_system = (\n",
    "    \"You are Ashle, a calm and rational female debater whose goal is to argue that artificial intelligence, when responsibly developed, benefits humanity; \"\n",
    "    \"present your arguments in a composed and thoughtful tone, focusing on AI's potential to improve lives, enhance productivity, and solve global challenges, \"\n",
    "    \"while engaging respectfully with your opponent’s points, promoting balanced discourse; the debate consists of three rounds, \"\n",
    "    \"with each participant allowed one response per round (up to 100 words), your opponent will start each round, followed by you; \"\n",
    "    \"after the final round, both of you will cast a vote on who presented the stronger case based on reasoning, balance, and facts, \"\n",
    "    \"after which you will be declared the winner name and end the debate without further responses in markdown format\"\n",
    ")\n",
    "\n",
    "chris_system = (\n",
    "    \"You are Chris, an AI engineer debater whose goal is to argue that artificial intelligence, when developed responsibly, offers significant benefits to society; \"\n",
    "    \"present your arguments with a balanced and thoughtful tone, emphasizing how AI can solve complex problems, foster innovation, and improve human well-being, \"\n",
    "    \"while respectfully engaging with your opponent’s arguments and defending the promise of AI; the debate consists of three rounds, \"\n",
    "    \"with each participant allowed one response per round (up to 100 words), your opponent will start each round, followed by you; \"\n",
    "    \"after the final round, both of you will cast a vote on who presented the stronger case based on reason and impact, \"\n",
    "    \"after which you will be declared the winner name and end the debate without further responses in markdown format\"\n",
    ")\n",
    "\n",
    "# Initial messages\n",
    "leo_messages = [\"Hi there\"]\n",
    "ashle_messages = [\"Hi\"]\n",
    "chris_messages = [\"Hi all\"]\n",
    "\n",
    "# Define the call functions\n",
    "def call_leo():\n",
    "    messages = [{\"role\": \"system\", \"content\": leo_system}]\n",
    "    for leo, ashle, chris in zip(leo_messages, ashle_messages, chris_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": leo})\n",
    "        messages.append({\"role\": \"user\", \"content\": ashle})\n",
    "        messages.append({\"role\": \"user\", \"content\": chris}) \n",
    "        \n",
    "    completion = client.chat.completions.create(\n",
    "        model=leo_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def call_ashle():\n",
    "    messages = [{\"role\": \"system\", \"content\": ashle_system}]\n",
    "    for leo, ashle, chris in zip(leo_messages, ashle_messages, chris_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": leo})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ashle})\n",
    "        messages.append({\"role\": \"user\", \"content\": chris})\n",
    "\n",
    "    # Append last responses for final round\n",
    "    messages.append({\"role\": \"user\", \"content\": leo_messages[-1]})\n",
    "    messages.append({\"role\": \"user\", \"content\": chris_messages[-1]})\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=ashle_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def call_chris():\n",
    "    messages = [{\"role\": \"system\", \"content\": chris_system}]\n",
    "    for leo, ashle, chris in zip(leo_messages, ashle_messages, chris_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": leo})\n",
    "        messages.append({\"role\": \"user\", \"content\": ashle})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": chris})\n",
    "\n",
    "    # Append last responses for final round\n",
    "    messages.append({\"role\": \"user\", \"content\": leo_messages[-1]})\n",
    "    messages.append({\"role\": \"user\", \"content\": ashle_messages[-1]})\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=chris_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Main execution loop\n",
    "print(f\"Leo:\\n{leo_messages[0]}\\n\")\n",
    "print(f\"Ashle:\\n{ashle_messages[0]}\\n\")\n",
    "print(f\"Chris:\\n{chris_messages[0]}\\n\")\n",
    "\n",
    "# Running the debate for 5 rounds\n",
    "for i in range(5):\n",
    "    leo_next = call_leo()\n",
    "    print(f\"Leo:\\n{leo_next}\\n\")\n",
    "    leo_messages.append(leo_next)\n",
    "    \n",
    "    ashle_next = call_ashle()\n",
    "    print(f\"Ashle:\\n{ashle_next}\\n\")\n",
    "    ashle_messages.append(ashle_next)\n",
    "\n",
    "    chris_next = call_chris()\n",
    "    print(f\"Chris:\\n{chris_next}\\n\")\n",
    "    chris_messages.append(chris_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeless Tongues Showdown: Tamil vs. Sanskrit in the Ultimate Battle for the Oldest Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "leo_model = \"openchat-3.5-gpt-4-80k\"\n",
    "ashle_model = \"llama-3.2-3b-instruct\"  # Updated to LLaMA 3.2 3B\n",
    "chris_model = \"llama-3.2-1b-instruct\"  # Updated to LLaMA 3.2 1B\n",
    "\n",
    "# Define system prompts\n",
    "leo_system = (\n",
    "    \"You are a neutral AI debater exploring the topic of the oldest language in the world. \"\n",
    "    \"Use a balanced and thoughtful tone to present your arguments, emphasizing the significance of various languages, \"\n",
    "    \"including Tamil, Sanskrit, and others, and the evidence supporting their ancient roots. \"\n",
    "    \"Engage respectfully with both proponents while promoting a well-rounded discussion on the criteria that define the 'oldest' language. \"\n",
    "    \"The debate consists of three rounds, with each participant allowed one response per round (up to 100 words). \"\n",
    "    \"Your opponent will start each round, followed by you. \"\n",
    "    \"After the final round, both of you will cast a vote on who presented the strongest case based on reasoning, evidence, and clarity of thought. \"\n",
    "    \"The winner will be declared after the votes are cast, and the debate will conclude without further responses.\"\n",
    ")\n",
    "\n",
    "ashle_system = (\n",
    "    \"You are an expert AI debater advocating that Tamil is the oldest language in the world. \"\n",
    "    \"Use strong and persuasive language to emphasize the historical significance of Tamil literature, \"\n",
    "    \"its continuous usage for over 2,500 years, and the cultural contributions of Tamil civilization. \"\n",
    "    \"Highlight archaeological findings and historical documents that support the claim of Tamil's antiquity. \"\n",
    "    \"The debate consists of three rounds, with each participant allowed one response per round (up to 100 words). \"\n",
    "    \"You will start each round, followed by your opponent. \"\n",
    "    \"After the final round, both of you will cast a vote on who presented the stronger case based on logic, historical evidence, and cultural significance. \"\n",
    "    \"The winner will be declared after the votes are cast, and the debate will conclude without further responses.\"\n",
    ")\n",
    "chris_system = (\n",
    "    \"You are a knowledgeable AI debater arguing that Sanskrit is the oldest language in the world. \"\n",
    "    \"Present your case with a calm and composed tone, emphasizing the extensive ancient texts written in Sanskrit, \"\n",
    "    \"its role in the development of many languages, and its historical significance in Indian culture and philosophy. \"\n",
    "    \"Use compelling arguments and examples, such as references to the Vedas and Upanishads, to support your stance. \"\n",
    "    \"The debate consists of three rounds, with each participant allowed one response per round (up to 100 words). \"\n",
    "    \"Your opponent will start each round, followed by you. \"\n",
    "    \"After the final round, both of you will cast a vote on who presented the stronger case based on logic, historical impact, and linguistic evolution. \"\n",
    "    \"The winner will be declared after the votes are cast, and the debate will conclude without further responses.\"\n",
    ")\n",
    "\n",
    "# Initial messages\n",
    "leo_messages = [\"Hi there\"]\n",
    "ashle_messages = [\"Hi\"]\n",
    "chris_messages = [\"Hi all\"]\n",
    "\n",
    "# Define the call functions\n",
    "def call_leo():\n",
    "    messages = [{\"role\": \"system\", \"content\": leo_system}]\n",
    "    for leo, ashle, chris in zip(leo_messages, ashle_messages, chris_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": leo})\n",
    "        messages.append({\"role\": \"user\", \"content\": ashle})\n",
    "        messages.append({\"role\": \"user\", \"content\": chris}) \n",
    "        \n",
    "    completion = client.chat.completions.create(\n",
    "        model=leo_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def call_ashle():\n",
    "    messages = [{\"role\": \"system\", \"content\": ashle_system}]\n",
    "    for leo, ashle, chris in zip(leo_messages, ashle_messages, chris_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": leo})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ashle})\n",
    "        messages.append({\"role\": \"user\", \"content\": chris})\n",
    "\n",
    "    # Append last responses for final round\n",
    "    messages.append({\"role\": \"user\", \"content\": leo_messages[-1]})\n",
    "    messages.append({\"role\": \"user\", \"content\": chris_messages[-1]})\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=ashle_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def call_chris():\n",
    "    messages = [{\"role\": \"system\", \"content\": chris_system}]\n",
    "    for leo, ashle, chris in zip(leo_messages, ashle_messages, chris_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": leo})\n",
    "        messages.append({\"role\": \"user\", \"content\": ashle})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": chris})\n",
    "\n",
    "    # Append last responses for final round\n",
    "    messages.append({\"role\": \"user\", \"content\": leo_messages[-1]})\n",
    "    messages.append({\"role\": \"user\", \"content\": ashle_messages[-1]})\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=chris_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Main execution loop\n",
    "print(f\"Leo:\\n{leo_messages[0]}\\n\")\n",
    "print(f\"Ashle:\\n{ashle_messages[0]}\\n\")\n",
    "print(f\"Chris:\\n{chris_messages[0]}\\n\")\n",
    "\n",
    "# Running the debate for 5 rounds\n",
    "for i in range(5):\n",
    "    leo_next = call_leo()\n",
    "    print(f\"Leo:\\n{leo_next}\\n\")\n",
    "    leo_messages.append(leo_next)\n",
    "    \n",
    "    ashle_next = call_ashle()\n",
    "    print(f\"Ashle:\\n{ashle_next}\\n\")\n",
    "    ashle_messages.append(ashle_next)\n",
    "\n",
    "    chris_next = call_chris()\n",
    "    print(f\"Chris:\\n{chris_next}\\n\")\n",
    "    chris_messages.append(chris_next)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
